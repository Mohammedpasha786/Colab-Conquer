{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_cell"
      },
      "source": [
        "# 🌸 Iris Dataset - Exploratory Data Analysis\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-username/iris-eda/blob/main/iris_eda.ipynb)\n",
        "\n",
        "## 📊 Project Overview\n",
        "\n",
        "This notebook performs comprehensive **Exploratory Data Analysis (EDA)** on the famous Iris dataset using Python's data science stack. The Iris dataset is a classic dataset in machine learning and statistics, containing measurements of iris flowers.\n",
        "\n",
        "### 🎯 Objectives\n",
        "- Load and inspect the Iris dataset\n",
        "- Perform statistical analysis (mean, median, standard deviation)\n",
        "- Create visualizations to understand data distribution\n",
        "- Analyze relationships between features\n",
        "- Explore species characteristics\n",
        "\n",
        "### 📈 Dataset Information\n",
        "- **Samples**: 150 iris flowers\n",
        "- **Features**: 4 numerical measurements\n",
        "- **Species**: 3 types (Setosa, Versicolor, Virginica)\n",
        "- **Source**: Classic machine learning dataset\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports_cell"
      },
      "source": [
        "## 📚 Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libraries"
      },
      "outputs": [],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Statistical functions\n",
        "from scipy import stats\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"✅ Libraries imported successfully!\")\n",
        "print(f\"📊 Pandas version: {pd.__version__}\")\n",
        "print(f\"📈 Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"🎨 Seaborn version: {sns.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_data_cell"
      },
      "source": [
        "## 📥 Load the Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from the provided URL\n",
        "url = 'https://bit.ly/4nejNue'\n",
        "\n",
        "try:\n",
        "    # Load data using pandas\n",
        "    df = pd.read_csv(url)\n",
        "    print(\"✅ Dataset loaded successfully!\")\n",
        "    print(f\"📊 Dataset shape: {df.shape}\")\n",
        "    print(f\"📈 Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading dataset: {e}\")\n",
        "    # Fallback: Load from sklearn if URL fails\n",
        "    from sklearn.datasets import load_iris\n",
        "    iris_sklearn = load_iris()\n",
        "    df = pd.DataFrame(iris_sklearn.data, columns=iris_sklearn.feature_names)\n",
        "    df['species'] = iris_sklearn.target_names[iris_sklearn.target]\n",
        "    print(\"✅ Loaded backup dataset from sklearn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_inspection_cell"
      },
      "source": [
        "## 🔍 Data Inspection and Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_inspection"
      },
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "print(\"🔍 First 5 rows of the dataset:\")\n",
        "print(\"=\" * 50)\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\n📊 Dataset Information:\")\n",
        "print(\"=\" * 30)\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n📈 Dataset Shape:\")\n",
        "print(f\"Rows: {df.shape[0]}\")\n",
        "print(f\"Columns: {df.shape[1]}\")\n",
        "\n",
        "print(\"\\n📋 Column Names:\")\n",
        "print(list(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_quality_check"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"🔍 Missing Values Check:\")\n",
        "print(\"=\" * 25)\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "if missing_values.sum() == 0:\n",
        "    print(\"✅ No missing values found!\")\n",
        "else:\n",
        "    print(f\"⚠️ Total missing values: {missing_values.sum()}\")\n",
        "\n",
        "print(\"\\n📊 Data Types:\")\n",
        "print(\"=\" * 15)\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n🏷️ Unique Values in Each Column:\")\n",
        "print(\"=\" * 35)\n",
        "for col in df.columns:\n",
        "    unique_count = df[col].nunique()\n",
        "    print(f\"{col}: {unique_count} unique values\")\n",
        "    if df[col].dtype == 'object' or unique_count < 10:\n",
        "        print(f\"   Values: {df[col].unique()}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "species_analysis_cell"
      },
      "source": [
        "## 🌺 Species Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "species_analysis"
      },
      "outputs": [],
      "source": [
        "# Identify the species column (it might have different names)\n",
        "species_col = None\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object' or 'species' in col.lower() or 'class' in col.lower():\n",
        "        species_col = col\n",
        "        break\n",
        "\n",
        "if species_col:\n",
        "    print(f\"🌸 Species Column Found: '{species_col}'\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Species distribution\n",
        "    species_counts = df[species_col].value_counts()\n",
        "    print(\"📊 Species Distribution:\")\n",
        "    print(species_counts)\n",
        "    \n",
        "    print(\"\\n📈 Species Percentages:\")\n",
        "    species_percentages = df[species_col].value_counts(normalize=True) * 100\n",
        "    for species, percentage in species_percentages.items():\n",
        "        print(f\"{species}: {percentage:.1f}%\")\n",
        "    \n",
        "    # Visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Bar plot\n",
        "    species_counts.plot(kind='bar', ax=ax1, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
        "    ax1.set_title('🌸 Species Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Species', fontweight='bold')\n",
        "    ax1.set_ylabel('Count', fontweight='bold')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Pie chart\n",
        "    species_counts.plot(kind='pie', ax=ax2, autopct='%1.1f%%', \n",
        "                       colors=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
        "    ax2.set_title('🌸 Species Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "    ax2.set_ylabel('')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠️ No species column found in the dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stats_analysis_cell"
      },
      "source": [
        "## 📊 Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "descriptive_stats"
      },
      "outputs": [],
      "source": [
        "# Get numerical columns only\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "print(\"📊 COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"📈 Numerical Features: {numerical_cols}\")\n",
        "print()\n",
        "\n",
        "# Descriptive statistics using .describe()\n",
        "print(\"📋 Descriptive Statistics (.describe()):\")\n",
        "print(\"=\" * 40)\n",
        "desc_stats = df[numerical_cols].describe()\n",
        "display(desc_stats.round(3))\n",
        "\n",
        "print(\"\\n📊 Individual Statistical Measures:\")\n",
        "print(\"=\" * 38)\n",
        "\n",
        "# Calculate individual statistics\n",
        "stats_summary = pd.DataFrame({\n",
        "    'Mean': df[numerical_cols].mean(),\n",
        "    'Median': df[numerical_cols].median(),\n",
        "    'Std Dev': df[numerical_cols].std(),\n",
        "    'Min': df[numerical_cols].min(),\n",
        "    'Max': df[numerical_cols].max(),\n",
        "    'Range': df[numerical_cols].max() - df[numerical_cols].min(),\n",
        "    'Variance': df[numerical_cols].var(),\n",
        "    'Skewness': df[numerical_cols].skew(),\n",
        "    'Kurtosis': df[numerical_cols].kurtosis()\n",
        "})\n",
        "\n",
        "display(stats_summary.round(3))\n",
        "\n",
        "# Print key insights\n",
        "print(\"\\n🔍 Key Statistical Insights:\")\n",
        "print(\"=\" * 30)\n",
        "for col in numerical_cols:\n",
        "    mean_val = df[col].mean()\n",
        "    median_val = df[col].median()\n",
        "    std_val = df[col].std()\n",
        "    skew_val = df[col].skew()\n",
        "    \n",
        "    print(f\"\\n📊 {col}:\")\n",
        "    print(f\"   • Mean: {mean_val:.3f} | Median: {median_val:.3f}\")\n",
        "    print(f\"   • Standard Deviation: {std_val:.3f}\")\n",
        "    print(f\"   • Coefficient of Variation: {(std_val/mean_val)*100:.1f}%\")\n",
        "    \n",
        "    if abs(skew_val) < 0.5:\n",
        "        skew_desc = \"approximately symmetric\"\n",
        "    elif skew_val > 0.5:\n",
        "        skew_desc = \"right-skewed (positive)\"\n",
        "    else:\n",
        "        skew_desc = \"left-skewed (negative)\"\n",
        "    \n",
        "    print(f\"   • Distribution: {skew_desc} (skew: {skew_val:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "correlation_cell"
      },
      "source": [
        "## 🔗 Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "correlation_analysis"
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "\n",
        "print(\"🔗 Correlation Matrix:\")\n",
        "print(\"=\" * 20)\n",
        "display(correlation_matrix.round(3))\n",
        "\n",
        "# Create correlation heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "sns.heatmap(correlation_matrix, \n",
        "            annot=True, \n",
        "            cmap='RdYlBu_r', \n",
        "            center=0,\n",
        "            square=True,\n",
        "            mask=mask,\n",
        "            cbar_kws={'label': 'Correlation Coefficient'},\n",
        "            fmt='.3f')\n",
        "plt.title('🔗 Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find strongest correlations\n",
        "print(\"\\n🔬 Strongest Correlations:\")\n",
        "print(\"=\" * 25)\n",
        "\n",
        "# Get upper triangle of correlation matrix\n",
        "correlation_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        col1 = correlation_matrix.columns[i]\n",
        "        col2 = correlation_matrix.columns[j]\n",
        "        corr_val = correlation_matrix.iloc[i, j]\n",
        "        correlation_pairs.append((col1, col2, corr_val))\n",
        "\n",
        "# Sort by absolute correlation value\n",
        "correlation_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
        "\n",
        "for col1, col2, corr in correlation_pairs:\n",
        "    strength = \"Strong\" if abs(corr) > 0.7 else \"Moderate\" if abs(corr) > 0.3 else \"Weak\"\n",
        "    direction = \"positive\" if corr > 0 else \"negative\"\n",
        "    print(f\"• {col1} ↔ {col2}: {corr:.3f} ({strength} {direction})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "histograms_cell"
      },
      "source": [
        "## 📊 Data Visualization - Histograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "histograms_basic"
      },
      "outputs": [],
      "source": [
        "# Create histograms for all numerical features\n",
        "num_features = len(numerical_cols)\n",
        "cols = 2\n",
        "rows = (num_features + cols - 1) // cols\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(15, 4*rows))\n",
        "fig.suptitle('📊 Distribution of All Numerical Features', \n",
        "             fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "# Flatten axes array for easier indexing\n",
        "if rows > 1:\n",
        "    axes = axes.flatten()\n",
        "elif cols > 1:\n",
        "    axes = axes\n",
        "else:\n",
        "    axes = [axes]\n",
        "\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', '#FF9FF3']\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    ax = axes[i] if num_features > 1 else axes\n",
        "    \n",
        "    # Create histogram\n",
        "    ax.hist(df[col], bins=20, alpha=0.7, color=colors[i % len(colors)], \n",
        "            edgecolor='black', linewidth=0.5)\n",
        "    \n",
        "    # Add vertical lines for mean and median\n",
        "    mean_val = df[col].mean()\n",
        "    median_val = df[col].median()\n",
        "    \n",
        "    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, \n",
        "               label=f'Mean: {mean_val:.2f}')\n",
        "    ax.axvline(median_val, color='blue', linestyle='--', linewidth=2, \n",
        "               label=f'Median: {median_val:.2f}')\n",
        "    \n",
        "    # Formatting\n",
        "    ax.set_title(f'Distribution of {col}', fontweight='bold', fontsize=12)\n",
        "    ax.set_xlabel(col, fontweight='bold')\n",
        "    ax.set_ylabel('Frequency', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add statistics text box\n",
        "    stats_text = f'Std: {df[col].std():.2f}\\nSkew: {df[col].skew():.2f}'\n",
        "    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
        "            verticalalignment='top', bbox=dict(boxstyle='round', \n",
        "            facecolor='white', alpha=0.8))\n",
        "\n",
        "# Hide empty subplots\n",
        "for i in range(num_features, len(axes)):\n",
        "    axes[i].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary of distributions\n",
        "print(\"\\n📊 Distribution Summary:\")\n",
        "print(\"=\" * 25)\n",
        "for col in numerical_cols:\n",
        "    skew_val = df[col].skew()\n",
        "    if abs(skew_val) < 0.5:\n",
        "        dist_shape = \"Normal/Symmetric\"\n",
        "    elif skew_val > 0.5:\n",
        "        dist_shape = \"Right-skewed\"\n",
        "    else:\n",
        "        dist_shape = \"Left-skewed\"\n",
        "    \n",
        "    print(f\"• {col}: {dist_shape} (skewness: {skew_val:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "species_histograms_cell"
      },
      "source": [
        "## 🌸 Species-wise Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "species_histograms"
      },
      "outputs": [],
      "source": [
        "if species_col:\n",
        "    # Create species-wise histograms\n",
        "    species_list = df[species_col].unique()\n",
        "    colors_species = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "    \n",
        "    fig, axes = plt.subplots(len(numerical_cols), 1, figsize=(15, 5*len(numerical_cols)))\n",
        "    fig.suptitle('🌸 Feature Distributions by Species', \n",
        "                 fontsize=20, fontweight='bold', y=1.02)\n",
        "    \n",
        "    if len(numerical_cols) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, col in enumerate(numerical_cols):\n",
        "        ax = axes[i]\n",
        "        \n",
        "        # Plot histogram for each species\n",
        "        for j, species in enumerate(species_list):\n",
        "            species_data = df[df[species_col] == species][col]\n",
        "            ax.hist(species_data, bins=15, alpha=0.6, \n",
        "                   label=f'{species} (n={len(species_data)})',\n",
        "                   color=colors_species[j % len(colors_species)])\n",
        "        \n",
        "        ax.set_title(f'Distribution of {col} by Species', \n",
        "                    fontweight='bold', fontsize=14)\n",
        "        ax.set_xlabel(col, fontweight='bold')\n",
        "        ax.set_ylabel('Frequency', fontweight='bold')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Species-wise statistics\n",
        "    print(\"\\n📊 Species-wise Statistical Summary:\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    species_stats = df.groupby(species_col)[numerical_cols].agg([\n",
        "        'mean', 'median', 'std', 'min', 'max'\n",
        "    ]).round(3)\n",
        "    \n",
        "    display(species_stats)\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ Species column not found - skipping species-wise analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced_viz_cell"
      },
      "source": [
        "## 📈 Advanced Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "box_plots"
      },
      "outputs": [],
      "source": [
        "# Box plots for outlier detection\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('📦 Box Plots - Outlier Detection', fontsize=18, fontweight='bold')\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(numerical_cols[:4]):  # Limit to 4 features\n",
        "    if species_col:\n",
        "        sns.boxplot(data=df, x=species_col, y=col, ax=axes[i])\n",
        "    else:\n",
        "        sns.boxplot(data=df, y=col, ax=axes[i])\n",
        "    \n",
        "    axes[i].set_title(f'Box Plot: {col}', fontweight='bold')\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Outlier detection\n",
        "print(\"\\n🔍 Outlier Detection (IQR Method):\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "outlier_summary = {}\n",
        "for col in numerical_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "    outlier_count = len(outliers)\n",
        "    outlier_percentage = (outlier_count / len(df)) * 100\n",
        "    \n",
        "    outlier_summary[col] = {\n",
        "        'count': outlier_count,\n",
        "        'percentage': outlier_percentage,\n",
        "        'lower_bound': lower_bound,\n",
        "        'upper_bound': upper_bound\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n📊 {col}:\")\n",
        "    print(f\"   • Outliers: {outlier_count} ({outlier_percentage:.1f}%)\")\n",
        "    print(f\"   • Valid range: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "    \n",
        "    if outlier_count > 0:\n",
